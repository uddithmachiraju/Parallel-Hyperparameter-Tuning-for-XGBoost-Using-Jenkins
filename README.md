# Parallel Hyperparameter Tuning for XGBoost using Jenkins

This project automates **parallel hyperparameter tuning** for **XGBoost** using **Jenkins**.  
It trains multiple models with different hyperparameters **simultaneously**, selects the best model, and deploys it using **Flask & Docker**.

---

## ðŸ“‚ Project Structure

Parallel_Hyperparameter_Tuning/ 
â”‚â”€â”€ .jenkins/                      # Jenkins pipeline setup
â”‚   â”œâ”€â”€ Jenkinsfile                # Jenkins CI/CD pipeline script
â”‚â”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ requirements.txt           # Required Dependencies 
â”‚â”€â”€ src/                           # Core ML training & selection scripts
â”‚   â”œâ”€â”€ model.py                   # Train XGBoost model
â”‚   â”œâ”€â”€ model_selection.py         # Select the best model based on loss 
â”‚â”€â”€ deployment/                    # Deployment files
â”‚   â”œâ”€â”€ app.py                     # Flask API for serving predictions
â”‚   â”œâ”€â”€ Dockerfile                 # Docker container setup
â”‚â”€â”€ tests/                         # Unit tests for ML training & API
â”‚   â”œâ”€â”€ test_app.py                # Tests XGBoost model training
â”‚   â”œâ”€â”€ test_xgb.py                # Tests model selection logic
â”‚â”€â”€ README.md                      # Project documentation
|â”€â”€ .gitignore                     # Git Ignore Files 

## âš¡ Features
**Parallel hyperparameter tuning** using Jenkins  
**XGBoost model training with different hyperparameters**  
**Automatic best model selection**  
**Flask API to serve predictions**  
**Dockerized deployment for easy scalability**